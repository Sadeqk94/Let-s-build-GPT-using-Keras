{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Generative Pretrained Transformer(GPT)\n",
    "\n",
    "**Author:** [Sadeq](https://github.com/Sadeqk94) [Kord](https://www.linkedin.com/in/sadeq-kord)<br>\n",
    "**Date created:** 2024/05/25<br>\n",
    "**Last modified:** 2024/05/25<br>\n",
    "**Description:** Generative Transformer for Tiny Shakespeare dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Introduction\n",
    "## Let's build GPT using Tensorflow and Keras!\n",
    "\n",
    "For those interested in learning about GPT models, there is an excellent video by [Andrej\n",
    "Karpathy](https://karpathy.ai/) titled [\"Let's build GPT: from scratch, in code, spelled\n",
    "out.\"](https://youtu.be/kCc8FmEb1nY?si=_l3tBiaZgq1NXwWW). In the video, Karpathy provides\n",
    "a detailed, step-by-step guide to building a GPT model using PyTorch.\n",
    "\n",
    "In this notebook, I present a TensorFlow/Keras version of Karpathy's implementation. This\n",
    "notebook follows the same principles and steps outlined in the video, allowing you to\n",
    "gain the same understanding and insights using TensorFlow/Keras. You can follow along\n",
    "with the video and refer to this notebook for the equivalent TensorFlow/Keras code,\n",
    "making it a valuable resource for anyone familiar with TensorFlow or looking to learn it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We begin by importing essential libraries for building and training our neural network\n",
    "model using TensorFlow and Keras. TensorFlow is a powerful deep learning library widely\n",
    "used for building various machine learning models, while Keras provides a high-level API\n",
    "simplifying the process of building neural networks. Additionally, we import NumPy for\n",
    "numerical computations and matplotlib for data visualization purposes.\n",
    "\n",
    "Furthermore, we define several hyperparameters essential for training our language model.\n",
    "These parameters include batch size, determining the number of independent sequences\n",
    "processed simultaneously during training, and block size, representing the maximum\n",
    "context length for predictions. Other parameters such as the number of iterations,\n",
    "learning rate, embedding size, number of heads, number of layers, and dropout rate are\n",
    "also specified. Setting seeds for NumPy and TensorFlow ensures reproducibility of results\n",
    "across different runs, a crucial aspect in machine learning experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "\n",
    "Epochs = max_iters // eval_interval\n",
    "\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Dataset prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In next step we should prepare data, therefore we download the `Tiny Shakespeare`\n",
    "dataset, a popular choice for language modeling tasks, to preprocess it for training a\n",
    "language model. The dataset is read from a text file, and we extract unique characters\n",
    "from it to construct the vocabulary. Using these characters, we create mappings from\n",
    "characters to integers and vice versa, facilitating the encoding and decoding of text\n",
    "data into numerical form, which is essential for training neural networks. Subsequently,\n",
    "we split the dataset into training and validation sets, with 90% of the data allocated\n",
    "for training and the remaining 10% for validation.\n",
    "\n",
    "Following data preparation, we define functions to generate batches of data for training\n",
    "and validation. These functions enable the creation of input-output pairs, where input\n",
    "sequences serve as context for predicting subsequent characters. Utilizing TensorFlow's\n",
    "data processing capabilities, we convert these batch generation functions into TensorFlow\n",
    "datasets, ensuring seamless integration with TensorFlow's training pipeline. These\n",
    "datasets are structured to produce batches of sequences, each consisting of a fixed\n",
    "number of characters, which will be used to train and validate our language model\n",
    "efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Download and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [\n",
    "    stoi[c] for c in s\n",
    "]  # encoder: take a string, output a list of integers\n",
    "decode = lambda l: \"\".join(\n",
    "    [itos[i] for i in l]\n",
    ")  # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = np.array(encode(text), dtype=np.int64)\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "# Data loading\n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = np.random.randint(0, len(data) - block_size, batch_size)\n",
    "    x = np.stack([data[i : i + block_size] for i in ix])\n",
    "    y = np.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Prepare train/val dataset\n",
    "def train_data_generator():\n",
    "    while True:\n",
    "        yield get_batch(\"train\")\n",
    "\n",
    "\n",
    "def val_data_generator():\n",
    "    while True:\n",
    "        yield get_batch(\"val\")\n",
    "\n",
    "\n",
    "train_data_generator = tf.data.Dataset.from_generator(\n",
    "    train_data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, block_size), dtype=tf.int64),\n",
    "        tf.TensorSpec(shape=(batch_size, block_size), dtype=tf.int64),\n",
    "    ),\n",
    ")\n",
    "\n",
    "val_data_generator = tf.data.Dataset.from_generator(\n",
    "    val_data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, block_size), dtype=tf.int64),\n",
    "        tf.TensorSpec(shape=(batch_size, block_size), dtype=tf.int64),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Language model Architucture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Our languge model is constructed based on 3 main parts/class. The first class,\n",
    "`FeedForward`, represents a simple feedforward neural network layer, a fundamental\n",
    "component of many deep learning architectures. In its `__init__` method, it initializes a\n",
    "sequential model consisting of two dense layers with ReLU activation functions and a\n",
    "dropout layer. The `call` method executes a forward pass through this network, taking an\n",
    "input tensor `x` and passing it through the sequential model, returning the output\n",
    "tensor.\n",
    "\n",
    "The `Block` class represents a single block of a transformer architecture. In its\n",
    "`__init__` method, it initializes the block with a multi-head self-attention layer\n",
    "(`sa`), a feedforward neural network layer (`ffwd`), and layer normalization layers\n",
    "(`ln1` and `ln2`). The `call` method executes a forward pass through this block. It first\n",
    "applies self-attention to the input tensor `x`, then adds the output to the input tensor,\n",
    "and passes it through the feedforward neural network layer. Finally, it returns the\n",
    "output tensor.\n",
    "It should be noted that, for language modeling we use `decoder attention` block and it\n",
    "has triangular masking that provides autoregressive settings and allows tokens to\n",
    "cominucate only with pervious tokens, you can see the explination\n",
    "[here](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4454s). when calling Keras\n",
    "MultiHeadAttention layer in `Block` class, we set `use_causal_mask=True` for this reason.\n",
    "\n",
    "The `BigramLanguageModel` class represents the entire language model architecture. It\n",
    "consists of embedding layers for token and positional embeddings, multiple transformer\n",
    "blocks, layer normalization, and a dense layer for output logits. The `call` method\n",
    "executes a forward pass through the model, applying token and positional embeddings,\n",
    "passing the input through the transformer blocks, and generating logits for the next\n",
    "token. It also computes the loss if targets are provided. Additionally, it includes\n",
    "methods `train_step` and `test_step` for training and evaluation steps, respectively, and\n",
    "a `generate` method for generating text given an input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "# %% Model components\n",
    "class FeedForward(layers.Layer):\n",
    "    \"\"\"A simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = models.Sequential(\n",
    "            [\n",
    "                layers.Dense(4 * n_embd, activation=\"relu\"),\n",
    "                layers.Dense(n_embd),\n",
    "                layers.Dropout(dropout),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(layers.Layer):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        self.sa = layers.MultiHeadAttention(\n",
    "            num_heads=n_head, key_dim=n_embd // n_head, dropout=dropout\n",
    "        )\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = layers.LayerNormalization()\n",
    "        self.ln2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        attn_output = self.sa(\n",
    "            self.ln1(x), self.ln1(x), use_causal_mask=True\n",
    "        )  # use causal mask to ensure each token can only see previous tokens\n",
    "        x = x + attn_output\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Bigram Language Model\n",
    "class BigramLanguageModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = layers.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = layers.Embedding(block_size, n_embd)\n",
    "        self.blocks = [Block(n_embd, n_head) for _ in range(n_layer)]\n",
    "        self.ln_f = layers.LayerNormalization()\n",
    "        self.lm_head = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            tf.range(T)[tf.newaxis, :]\n",
    "        )  # initially (T,C) adding new axis and get # (1,T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        for block in self.blocks:  # (B,T,C)\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "\n",
    "        logits_flat = tf.reshape(logits, [-1, logits.shape[-1]])\n",
    "        targets_flat = tf.reshape(targets, [-1])\n",
    "        loss = keras.losses.sparse_categorical_crossentropy(\n",
    "            targets_flat, logits_flat, from_logits=True\n",
    "        )\n",
    "        return logits, tf.reduce_mean(loss)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, loss = self(x, y)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        logits, loss = self(x, y)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = tf.random.categorical(logits, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = tf.concat([idx, idx_next], axis=1)  # (B, T+1)\n",
    "        return idx\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now its time to initialize the language model (BigramLanguageModel) and see the number of\n",
    "trainable parameters in the model using the count_params() method. This gives us insight\n",
    "into the complexity of the model and the memory requirements for training.\n",
    "\n",
    "After initializing the model, we compile it using the Adam optimizer with a specified\n",
    "learning rate. Compilation involves setting up the model for training, including\n",
    "specifying the loss function and optimization algorithm.\n",
    "\n",
    "Next, we train the model using the fit method. We specify the training data generator\n",
    "(train_data_generator), the number of epochs (Epochs), the evaluation interval\n",
    "(eval_interval), the validation data generator (val_data_generator), and the number of\n",
    "validation steps (eval_iters). During training, the model learns to minimize the loss\n",
    "function on the training data while monitoring its performance on the validation data.\n",
    "\n",
    "Finally, we plot the learning curves using Matplotlib. The learning curves show the\n",
    "training and validation loss as a function of the number of epochs. This visualization\n",
    "helps us understand how well the model is learning over time and whether it is\n",
    "overfitting or underfitting. By observing the trends in the loss curves, we can make\n",
    "informed decisions about model training and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Initialize the model train and plotting loss curves\n",
    "model = BigramLanguageModel()\n",
    "# print the number of parameters in the model\n",
    "model.build((batch_size, block_size))\n",
    "print(\"Number of trainable parameters:\", model.count_params())\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate))\n",
    "\n",
    "# Train the model\n",
    "Hist = model.fit(\n",
    "    train_data_generator,\n",
    "    epochs=Epochs,\n",
    "    steps_per_epoch=eval_interval,\n",
    "    validation_data=val_data_generator,\n",
    "    validation_steps=eval_iters,\n",
    ")\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    np.arange(1, Epochs + 1),\n",
    "    np.vstack((Hist.history[\"loss\"], Hist.history[\"val_loss\"])).T,\n",
    ")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"train_loss\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Geneating Shakespeare-like text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "And finally, we generate text using the trained language model (BigramLanguageModel). We\n",
    "initialize the generation process by providing an initial context, which is represented\n",
    "as an array of zeros with shape (1, 1). This context serves as the starting point for\n",
    "text generation.\n",
    "\n",
    "We then use the generate method of the model to generate a sequence of tokens. We specify\n",
    "the maximum number of new tokens to generate (max_new_tokens) as 2000. The model\n",
    "iteratively predicts the next token based on the provided context and appends it to the\n",
    "sequence.\n",
    "\n",
    "Once the text generation process is complete, we decode the generated sequence of tokens\n",
    "into human-readable text using the decode function. This function maps each token index\n",
    "back to its corresponding character in the vocabulary.\n",
    "\n",
    "Finally, we print the generated text to the console, allowing us to inspect the output of\n",
    "the language model and assess its quality. This text generation process demonstrates the\n",
    "model's ability to generate coherent and contextually relevant text based on the patterns\n",
    "learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Generate text\n",
    "context = np.zeros((1, 1), dtype=np.int64)\n",
    "generated = model.generate(context, max_new_tokens=2000)\n",
    "print(decode(generated[0].numpy().tolist()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C:\\Users\\Sadeq\\Desktop\\gpt\\keras-io\\examples\\nlp\\Generative_Pretrained_Transformer(GPT)",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}